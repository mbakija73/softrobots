{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpobX6mJMEIk"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "vlJqI5zyMD0Z",
        "outputId": "a865c2c3-9cf7-4645-faa7-49384b70a3a9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.optim import Adam\n",
        "from torch.nn import Upsample\n",
        "import torch.nn.functional as F\n",
        "from utils import soft_update, hard_update\n",
        "from model2 import GaussianPolicy, QNetwork, VNetwork"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdSY9o6vMKun"
      },
      "source": [
        "#Original Soft Actor Critic (SAC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_Fvy-n4L3Py"
      },
      "outputs": [],
      "source": [
        "class SAC(object):\n",
        "    def __init__(self, num_inputs, action_space, args):\n",
        "        self.args = args\n",
        "        self.num_inputs = num_inputs\n",
        "        self.gamma = args.gamma\n",
        "        self.tau = args.tau\n",
        "        self.alpha = args.alpha\n",
        "        self.action_res = args.action_res\n",
        "        self.target_update_interval = args.target_update_interval\n",
        "        self.automatic_entropy_tuning = args.automatic_entropy_tuning\n",
        "        self.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "        self.exp_upsample_list = [Upsample(scale_factor=i, mode='bicubic', align_corners=True) for i in [1, 2, 4, 8]]\n",
        "\n",
        "        # for reward normalization\n",
        "        self.momentum = args.momentum\n",
        "        self.mean = 0.0\n",
        "        self.var = 1.0\n",
        "\n",
        "        self.last_state_batch = None\n",
        "        self.old_std = None\n",
        "\n",
        "        # critic\n",
        "        self.upsampled_action_res = args.action_res * args.action_res_resize\n",
        "        self.critic = QNetwork(num_inputs, self.action_res,\\\n",
        "            self.upsampled_action_res, args.hidden_size).to(device=self.device)\n",
        "        self.critic_target = QNetwork(num_inputs, self.action_res,\\\n",
        "            self.upsampled_action_res, args.hidden_size).to(device=self.device)\n",
        "        self.critic_optim = Adam(self.critic.parameters(), lr=args.lr)\n",
        "        hard_update(self.critic_target, self.critic)\n",
        "\n",
        "        # actor\n",
        "        self.policy = GaussianPolicy(num_inputs, self.action_res,\\\n",
        "            self.upsampled_action_res, args.residual, args.coarse2fine_bias).to(device=self.device)\n",
        "        self.policy_optim = Adam(self.policy.parameters(), lr=args.lr)\n",
        "\n",
        "        # auto alpha\n",
        "        if self.automatic_entropy_tuning:\n",
        "            self.target_entropy = -torch.Tensor([action_space.shape[0]]).to(self.device).item()\n",
        "            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
        "            self.alpha_optim = Adam([self.log_alpha], lr=args.lr)\n",
        "\n",
        "    def select_action(self, state, coarse_action=None, task=None):\n",
        "        state = (torch.FloatTensor(state) / 255.0 * 2.0 - 1.0).to(self.device).unsqueeze(0)\n",
        "        if coarse_action is not None:\n",
        "            coarse_action = torch.FloatTensor(coarse_action).to(self.device).unsqueeze(0)\n",
        "        if task is None or \"shapematch\" not in task:\n",
        "            action, _, _, _, mask = self.policy.sample(state, coarse_action)\n",
        "        else:\n",
        "            _, _, action, _, mask = self.policy.sample(state, coarse_action)\n",
        "            action = torch.tanh(action)\n",
        "        action = action.detach().cpu().numpy()[0]\n",
        "        if coarse_action is not None:\n",
        "            mask = mask.detach().cpu().numpy()[0]\n",
        "        return action, mask, None\n",
        "\n",
        "    def select_coarse_action(self, state, coarse_action=None, task=None):\n",
        "        state = (torch.FloatTensor(state) / 255.0 * 2.0 - 1.0).to(self.device).unsqueeze(0)\n",
        "        if coarse_action is not None:\n",
        "            coarse_action = torch.FloatTensor(coarse_action).to(self.device).unsqueeze(0)\n",
        "        if task is None or \"shapematch\" not in task:\n",
        "            action, _, mean, std, _ = self.coarse_policy.sample(state, coarse_action)\n",
        "            action = action.detach().cpu().numpy()[0]\n",
        "            mean = mean.detach().cpu().numpy()[0]\n",
        "            std = std.detach().cpu().numpy()[0]\n",
        "            return action, mean, std\n",
        "        else:\n",
        "            _, _, action, _, _ = self.coarse_policy.sample(state, coarse_action)\n",
        "            action = torch.tanh(action)\n",
        "            action = action.detach().cpu().numpy()[0]\n",
        "            return action, None, None\n",
        "\n",
        "    def reward_normalization(self, rewards):\n",
        "        # update mean and var for reward normalization\n",
        "        batch_mean = torch.mean(rewards)\n",
        "        batch_var = torch.var(rewards)\n",
        "        self.mean = self.momentum * self.mean + (1 - self.momentum) * batch_mean\n",
        "        self.var = self.momentum * self.var + (1 - self.momentum) * batch_var\n",
        "        std = torch.sqrt(self.var)\n",
        "        normalized_rewards = (rewards - self.mean) / (std + 1e-8)\n",
        "        return normalized_rewards\n",
        "\n",
        "\n",
        "    def update_parameters(self, memory, updates, start=False):\n",
        "        # sample a batch from memory\n",
        "        (\n",
        "            state_batch,\n",
        "            action_batch,\n",
        "            reward_batch,\n",
        "            next_state_batch,\n",
        "            mask_batch\n",
        "        ) = memory.sample(self.args.batch_size)\n",
        "        state_batch = (torch.FloatTensor(state_batch) / 255.0 * 2.0 - 1.0).to(self.device)\n",
        "        next_state_batch = (torch.FloatTensor(next_state_batch) / 255.0 * 2.0 - 1.0).to(self.device)\n",
        "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
        "        reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
        "        mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
        "        # normalize rewards\n",
        "        reward_batch = self.reward_normalization(reward_batch)\n",
        "        # # SAC\n",
        "        # critic\n",
        "        with torch.no_grad():\n",
        "            if self.args.residual:\n",
        "                next_original_action = self.upsample_coarse_action(next_state_batch)\n",
        "                next_state_pi, next_state_log_pi, _, _, mask = self.policy.sample(next_state_batch, next_original_action)\n",
        "                next_state_pi = mask * next_state_pi + (1 - mask) * next_original_action.reshape(self.args.batch_size, -1)\n",
        "            else:\n",
        "                next_state_pi, next_state_log_pi, _, _, _ = self.policy.sample(next_state_batch)\n",
        "                #print(\"next_state_log_pi stats: min\", next_state_log_pi.min().item(), \"max\", next_state_log_pi.max().item(), \"mean\", next_state_log_pi.mean().item())\n",
        "            qf1_next_target, qf2_next_target = self.critic_target(next_state_batch, next_state_pi)\n",
        "            # only force fine policy to explore\n",
        "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - self.alpha * next_state_log_pi\n",
        "            next_q_value = reward_batch + mask_batch * self.gamma * (min_qf_next_target)\n",
        "        # two Q-functions to mitigate positive bias in the policy improvement step\n",
        "        # JQ = ùîº(st,at)~D[0.5(Q(st,at) - r(st,at) - Œ≥(ùîºst+1~p[V(st+1)]))^2]\n",
        "        qf1, qf2 = self.critic(state_batch, action_batch)\n",
        "        qf1_loss = F.mse_loss(qf1, next_q_value)\n",
        "        qf2_loss = F.mse_loss(qf2, next_q_value)\n",
        "        qf_loss = qf1_loss + qf2_loss\n",
        "        # update critic\n",
        "        self.critic_optim.zero_grad()\n",
        "        qf_loss.backward()\n",
        "        for params in self.critic.parameters():\n",
        "            torch.nn.utils.clip_grad_norm_(params, max_norm=10)\n",
        "        self.critic_optim.step()\n",
        "\n",
        "        # if(self.last_state_batch != None):\n",
        "        #     print(\"state dif\", (self.last_state_batch - state_batch).mean().item(), \"max\", (self.last_state_batch - state_batch).max().item(), \"min \", (self.last_state_batch - state_batch).min().item())\n",
        "        # self.last_state_batch = state_batch\n",
        "        #actor\n",
        "        if self.args.residual:\n",
        "            with torch.no_grad():\n",
        "                coarse_action = self.upsample_coarse_action(state_batch)\n",
        "            pi, log_pi, _, std, mask = self.policy.sample(state_batch, coarse_action)\n",
        "            pi = mask * pi + (1 - mask) * coarse_action.reshape(self.args.batch_size, -1)\n",
        "        else:\n",
        "            pi, log_pi, _, std, _ = self.policy.sample(state_batch)\n",
        "        qf1_pi, qf2_pi = self.critic(state_batch, pi)\n",
        "        min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
        "        # JœÄ = ùîºst‚àºD,Œµt‚àºN[Œ± * logœÄ(f(Œµt;st)|st) ‚àí Q(st,f(Œµt;st))]\n",
        "        policy_loss_ = ((self.alpha * log_pi) - min_qf_pi).mean()\n",
        "        if self.args.residual:\n",
        "            # regularize mask to be close to 0\n",
        "            mask_regularize_loss = self.args.coarse2fine_penalty *\\\n",
        "                torch.norm(mask.reshape(mask.shape[0], -1), dim=1).mean() / self.args.action_res\n",
        "            policy_loss = policy_loss_ + mask_regularize_loss\n",
        "        else:\n",
        "            policy_loss = policy_loss_\n",
        "            mask_regularize_loss = torch.zeros(1).to(self.device)\n",
        "        # update policy\n",
        "        self.policy_optim.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        for params in self.policy.parameters():\n",
        "            torch.nn.utils.clip_grad_norm_(params, max_norm=10)\n",
        "\n",
        "        self.policy_optim.step()\n",
        "\n",
        "        entropy, std = self.policy.entropy(state_batch)\n",
        "\n",
        "        if self.automatic_entropy_tuning:\n",
        "            alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
        "            self.alpha_optim.zero_grad()\n",
        "            alpha_loss.backward()\n",
        "            self.alpha_optim.step()\n",
        "\n",
        "        if updates % self.target_update_interval == 0:\n",
        "            soft_update(self.critic_target, self.critic, self.tau)\n",
        "\n",
        "        return qf1_loss.item(), qf2_loss.item(), policy_loss.item(),-torch.mean(log_pi).item(), self.alpha,\\\n",
        "            entropy.mean().item(), mask_regularize_loss.item() #torch.norm(std.reshape(std.shape[0], -1), dim=1).mean().item() / (self.args.action_res**2)\n",
        "\n",
        "    def upsample_coarse_action(self, state_batch):\n",
        "        coarse_pi, _, _, _, _ = self.coarse_policy.sample(state_batch)\n",
        "        coarse_pi = coarse_pi.reshape(self.args.batch_size, 2,\\\n",
        "            self.args.coarse_action_res, self.args.coarse_action_res)\n",
        "        return self.exp_upsample_list[int(math.log2(self.args.action_res / self.args.coarse_action_res))](coarse_pi)\n",
        "\n",
        "    # save model parameters\n",
        "    def save_model(self, filename):\n",
        "        checkpoint = {\n",
        "            \"mean\": self.mean,\n",
        "            \"var\": self.var,\n",
        "            \"policy\": self.policy.state_dict(),\n",
        "            \"critic\": self.critic.state_dict(),\n",
        "            \"critic_optim\": self.critic_optim.state_dict(),\n",
        "            \"policy_optim\": self.policy_optim.state_dict(),\n",
        "        }\n",
        "        torch.save(checkpoint, filename + \".pth\")\n",
        "\n",
        "    # load model parameters\n",
        "    def load_model(self, filename, for_train=False):\n",
        "        print('Loading models from {}...'.format(filename))\n",
        "        checkpoint = torch.load(filename)\n",
        "        mean = checkpoint.get(\"mean\")\n",
        "        var = checkpoint.get(\"var\")\n",
        "        if mean is not None:\n",
        "            self.mean = mean\n",
        "        if var is not None:\n",
        "            self.var = var\n",
        "        self.policy.load_state_dict(checkpoint[\"policy\"])\n",
        "        self.critic.load_state_dict(checkpoint[\"critic\"])\n",
        "        if for_train:\n",
        "            self.policy_optim.load_state_dict(checkpoint[\"policy_optim\"])\n",
        "            self.critic_optim.load_state_dict(checkpoint[\"critic_optim\"])\n",
        "\n",
        "    # load coarse model\n",
        "    def load_coarse_model(self, filename, action_res):\n",
        "        print('Loading coarse models from {}...'.format(filename))\n",
        "        self.coarse_policy = GaussianPolicy(self.num_inputs, action_res,\\\n",
        "            self.upsampled_action_res, False, self.args.coarse2fine_bias).to(self.device)\n",
        "        checkpoint = torch.load(filename)\n",
        "        self.coarse_policy.load_state_dict(checkpoint[\"policy\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmu6A0w1B2sq"
      },
      "source": [
        "#PPO (Proximal Policy Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMzyDNsZB83g"
      },
      "outputs": [],
      "source": [
        "#ctrl /\n",
        "\n",
        "class PPO(object):\n",
        "    def __init__(self, num_inputs, action_space, args):\n",
        "        self.args = args\n",
        "        self.num_inputs = num_inputs #env.observation_space.shape[0]\n",
        "        self.gamma = args.gamma\n",
        "        self.alpha = args.alpha\n",
        "        self.epsilon = args.epsilon\n",
        "        self.action_res = args.action_res\n",
        "\n",
        "        self.upsampled_action_res = args.action_res * args.action_res_resize\n",
        "        self.automatic_entropy_tuning = args.automatic_entropy_tuning\n",
        "        self.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "        # for reward normalization\n",
        "        self.momentum = args.momentum\n",
        "        self.mean = 0.0\n",
        "        self.var = 1.0\n",
        "\n",
        "        self.entropy_coeff = self.args.entropy_coeff\n",
        "\n",
        "        #Value\n",
        "        self.values = VNetwork(num_inputs, self.action_res, args.hidden_size).to(device=self.device)\n",
        "        self.value_optim = Adam(self.values.parameters(), lr=args.value_lr)\n",
        "\n",
        "        # policy\n",
        "        self.policy = GaussianPolicy(num_inputs, self.action_res,\\\n",
        "            self.upsampled_action_res, args.residual, args.coarse2fine_bias,nl1=args.nl1,nl2=args.nl2).to(device=self.device)\n",
        "        lr=3e-4\n",
        "        self.policy_optim = Adam(self.policy.parameters(), lr=args.policy_lr)\n",
        "\n",
        "        #old estate batch for comparisson\n",
        "        self.old_state_batch = None\n",
        "        self.old_std = None\n",
        "\n",
        "        print(\"weight decay: \", args.weight_decay)\n",
        "        print(\"policy lr: \", args.policy_lr)\n",
        "        print(\"value lr: \", args.value_lr)\n",
        "        print(\"self.args.log_sum_exp: \",args.log_sum_exp)\n",
        "        print(\"adv_norm: \",args.adv_norm)\n",
        "        print(\"entropy_coeff: \",args.entropy_coeff)\n",
        "        print(\"entropy_decay: \",args.entropy_decay)\n",
        "        print(\"Gradient clip policy: \",args.max_norm_p)\n",
        "        print(\"Gradient clip value: \",args.max_norm_v)\n",
        "        print(\"Std initial weight noise: \",args.nl1)\n",
        "        print(\"Mean initial weights noise: \",args.nl2)\n",
        "        print(\"TD error for value loss calculation\", args.td_error)\n",
        "\n",
        "\n",
        "    def select_action(self, state, coarse_action=None, task=None):\n",
        "        mean = None\n",
        "        std = None\n",
        "        state = (torch.FloatTensor(state) / 255.0 * 2.0 - 1.0).to(self.device).unsqueeze(0)\n",
        "        if coarse_action is not None:\n",
        "            coarse_action = torch.FloatTensor(coarse_action).to(self.device).unsqueeze(0)\n",
        "        if task is None or \"shapematch\" not in task:\n",
        "            action, log_prob, mean, std, mask = self.policy.sample(state, coarse_action)\n",
        "            #print(\"log prob \", log_prob)\n",
        "        else:\n",
        "            _, log_prob, action, _, mask = self.policy.sample(state, coarse_action)\n",
        "            action = torch.tanh(action)\n",
        "        action = action.detach().cpu().numpy()[0]\n",
        "        if coarse_action is not None:\n",
        "            mask = mask.detach().cpu().numpy()[0]\n",
        "\n",
        "        return action, mask , log_prob, mean, std\n",
        "\n",
        "    def log_prob(self, state, action, task=None):\n",
        "        state = (torch.FloatTensor(state) / 255.0 * 2.0 - 1.0).to(self.device).unsqueeze(0)\n",
        "        action = torch.FloatTensor(action).to(self.device).unsqueeze(0)\n",
        "        log_pi = self.policy.log_prob(state,action)\n",
        "        return log_pi\n",
        "\n",
        "\n",
        "    def reward_normalization(self, rewards):\n",
        "        # update mean and var for reward normalization\n",
        "        batch_mean = torch.mean(rewards)\n",
        "        batch_var = torch.var(rewards)\n",
        "        self.mean = self.momentum * self.mean + (1 - self.momentum) * batch_mean\n",
        "        self.var = self.momentum * self.var + (1 - self.momentum) * batch_var\n",
        "        std = torch.sqrt(self.var)\n",
        "        normalized_rewards = (rewards - self.mean) / (std + 1e-8)\n",
        "        return normalized_rewards\n",
        "\n",
        "    def advantage_normalization(self, advantages):\n",
        "        # update mean and var for reward normalization\n",
        "        batch_mean = torch.mean(advantages)\n",
        "        batch_var = torch.var(advantages)\n",
        "        std = torch.sqrt(batch_var)\n",
        "        #normalized_advantages = (advantages - batch_mean) / (std + 1e-8)\n",
        "        normalized_advantages = (advantages) / (std + 1e-8)\n",
        "        return (normalized_advantages * 10)\n",
        "\n",
        "    def precompute_batches(self, memory, num_batches):\n",
        "      batches = []\n",
        "\n",
        "      for _ in range(num_batches):\n",
        "          (\n",
        "              state_batch,\n",
        "              action_batch,\n",
        "              reward_batch,\n",
        "              next_state_batch,\n",
        "              mask_batch,\n",
        "              old_log_prob_batch,\n",
        "              reward_l_batch\n",
        "          )= memory.sample(self.args.batch_size)\n",
        "          state_batch = (torch.FloatTensor(state_batch) / 255.0 * 2.0 - 1.0).to(self.device)\n",
        "\n",
        "          action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
        "          with torch.no_grad():\n",
        "            log_prob, _ = self.policy.log_prob(state_batch, action_batch)\n",
        "\n",
        "          # Store everything needed for later updates\n",
        "          batches.append({\n",
        "              'state_batch': state_batch,\n",
        "              'action_batch': action_batch,\n",
        "              'reward_batch': torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1),\n",
        "              'next_state_batch': (torch.FloatTensor(next_state_batch) / 255.0 * 2.0 - 1.0).to(self.device),\n",
        "              'mask_batch': torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1),\n",
        "              'old_log_prob_batch': log_prob,\n",
        "              'reward_l_batch': torch.FloatTensor(reward_l_batch).to(self.device).unsqueeze(1),\n",
        "          })\n",
        "      return batches\n",
        "\n",
        "    def log_sum_exp(self,log_probs):\n",
        "      max_log_prob = torch.max(log_probs)  # Use torch.max for PyTorch tensors\n",
        "      stable_log_probs = log_probs - max_log_prob  # Stabilize the log probabilities\n",
        "      sum_exp = torch.sum(torch.exp(stable_log_probs))  # Sum of the exponentials\n",
        "      return max_log_prob + torch.log(sum_exp)  # Log of summed exponentials\n",
        "\n",
        "\n",
        "    def update_parameters(self, memory, batch,start):\n",
        "        #batches = self.precompute_batches(memory, 15)\n",
        "          torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "          state_batch = batch['state_batch']\n",
        "          action_batch = batch['action_batch']\n",
        "          reward_batch = batch['reward_batch']\n",
        "          next_state_batch = batch['next_state_batch']\n",
        "          mask_batch = batch['mask_batch']\n",
        "          old_log_prob_batch = batch['old_log_prob_batch']\n",
        "          reward_l_batch = batch['reward_l_batch']\n",
        "          #reward_l_batch = self.reward_normalization(reward_l_batch)\n",
        "          \n",
        "          #AÀÜt = Rt (Œª) ‚àíV (st ).\n",
        "          est_values = self.values(state_batch)\n",
        "          advantages_batch = reward_l_batch - est_values #torch.exp(reward_l_batch - values_batch)\n",
        "\n",
        "          log_pi, std  = (self.policy.log_prob(state_batch, action_batch,start=start))\n",
        "\n",
        "          if(self.args.loud):\n",
        "            if(self.old_std != None):\n",
        "              #print(\"shape\", std.shape)\n",
        "              print(\"(self.old_std - std) stats: min\", (self.old_std - std).min().item(), \"max\", (self.old_std - std).max().item(), \"mean\", abs((self.old_std - std)).mean().item())\n",
        "              #print((self.old_std - std).mode())\n",
        "            self.old_std = std\n",
        "            if(self.old_state_batch != None):\n",
        "              print(\"(state_batch - old_state_batch) stats: min\", (state_batch - self.old_state_batch).min().item(), \"max\", (state_batch - self.old_state_batch).max().item(), \"mean\", (state_batch - self.old_state_batch).mean().item())\n",
        "            self.old_state_batch = state_batch\n",
        "            print(\"advantages_batch stats: min\", advantages_batch.min().item(), \"max\", advantages_batch.max().item(), \"mean\", advantages_batch.mean().item())\n",
        "            print(\"(log_pi - old_log_prob_batch) stats: min\", (log_pi - old_log_prob_batch).min().item(), \"max\", (log_pi - old_log_prob_batch).max().item(), \"mean\", (log_pi - old_log_prob_batch).mean().item())\n",
        "            print(\"log_pi stats: min\", log_pi.min().item(), \"max\", log_pi.max().item(), \"mean\", log_pi.mean().item())\n",
        "          # Step 2: Normalize the log probabilities\n",
        "          if(self.args.log_sum_exp):\n",
        "            log_sum_exp = torch.logsumexp(log_pi, dim=0)\n",
        "            log_pi = log_pi - log_sum_exp\n",
        "            old_log_sum_exp = torch.logsumexp(old_log_prob_batch, dim=0)\n",
        "            old_log_prob_batch = old_log_prob_batch - old_log_sum_exp\n",
        "          if(self.args.adv_norm):\n",
        "            advantages_batch = self.advantage_normalization(advantages_batch)\n",
        "          #print(\"advantages_batch: min\", advantages_batch.min().item(), \"max\", advantages_batch.max().item(), \"mean\", advantages_batch.mean().item())\n",
        "          #****you can also normalize the advantages i guess but wait probably dont do that I think\n",
        "\n",
        "          diff_log_prob = log_pi - old_log_prob_batch\n",
        "          diff_log_prob = torch.clamp(diff_log_prob, -10.0, 10.0)\n",
        "          # # Normalize the differences\n",
        "          # #this is a bad idea cause like one will be not 0 and that one will have a huge thing but ig will be clamped idk\n",
        "\n",
        "          # Apply exponential function to the normalized differences\n",
        "          ratios = torch.exp(diff_log_prob)\n",
        "          if(self.args.loud):\n",
        "            print(\"diff_log_prob stats: min\", diff_log_prob.min().item(), \"max\", diff_log_prob.max().item(), \"mean\", diff_log_prob.mean().item())\n",
        "            print(\" ratios stats: min\", ratios.min().item(), \"max\", ratios.max().item(), \"mean\", ratios.mean().item())\n",
        "\n",
        "          par1 = ratios * advantages_batch\n",
        "          par2 = torch.clamp(ratios, 1.0 - self.epsilon, 1.0 + self.epsilon) * advantages_batch\n",
        "          policy_loss = -torch.min(par1, par2).mean()\n",
        "\n",
        "          entropy, _ = self.policy.entropy(state_batch)\n",
        "\n",
        "          # # Add entropy bonus to the policy loss\n",
        "          self.entropy_coeff = self.entropy_coeff * self.args.entropy_decay #0.999#self.args.entropy_coeff\n",
        "          policy_loss = policy_loss - (self.entropy_coeff * entropy)#(100 * std.std())#(self.entropy_coeff * entropy)\n",
        "          #print((-torch.min(par1, par2)).shape, entropy.mean(dim=1, keepdim=True).shape)\n",
        "          policy_loss = (-torch.min(par1, par2) - (self.entropy_coeff * entropy.mean(dim=1, keepdim=True))).mean()\n",
        "          if(self.args.loud):\n",
        "            print(\"entropy\",entropy.mean().item())\n",
        "            print(\"policy loss\", policy_loss.item())\n",
        "            #print(\"entropy loss\", (self.args.entropy_coeff * entropy).item())\n",
        "            print(\"Std std\", std.std().item())\n",
        "\n",
        "\n",
        "          # update policy\n",
        "          self.policy_optim.zero_grad()\n",
        "          policy_loss.backward()\n",
        "\n",
        "          for params in self.policy.parameters():\n",
        "              torch.nn.utils.clip_grad_norm_(params, max_norm=self.args.max_norm_p)\n",
        "          self.policy_optim.step()\n",
        "\n",
        "          #update value function\n",
        "          if(self.args.rl_norm):\n",
        "              reward_l_batch = self.reward_normalization(reward_l_batch)\n",
        "\n",
        "          est_values = self.values(state_batch)\n",
        "          # if(self.args.td_error):\n",
        "          #   V_next = self.values(next_state_batch)\n",
        "          #   v_loss = (reward_l_batch + 0.95 *(V_next - est_values)).pow(2).mean() # Compute TD error\n",
        "          # else:\n",
        "          v_loss = ((est_values - reward_l_batch) ** 2).mean() / 1000\n",
        "\n",
        "          if(self.args.loud):\n",
        "            print(\"value loss\", v_loss.item())\n",
        "            print(\"est values\", est_values.mean().item())\n",
        "            print(\"reward batchs\",reward_l_batch.mean().item())\n",
        "\n",
        "          self.value_optim.zero_grad()\n",
        "          v_loss.backward()\n",
        "          for params in self.values.parameters():\n",
        "              torch.nn.utils.clip_grad_norm_(params, max_norm= self.args.max_norm_v)\n",
        "          self.value_optim.step()\n",
        "\n",
        "          return v_loss.item(), policy_loss.item(), entropy.mean().item(), (abs(self.entropy_coeff * entropy.mean())/(abs(self.entropy_coeff * entropy.mean()) + abs(policy_loss)))#ratios.min().item()#torch.norm(std.reshape(std.shape[0], -1), dim=1).mean().item() / (self.args.action_res**2)\n",
        "\n",
        "    # save model parameters\n",
        "    def save_model(self, filename):\n",
        "        checkpoint = {\n",
        "            \"mean\": self.mean,\n",
        "            \"var\": self.var,\n",
        "            \"policy\": self.policy.state_dict(),\n",
        "            \"values\": self.values.state_dict(),\n",
        "            \"value_optim\": self.value_optim.state_dict(),\n",
        "            \"policy_optim\": self.policy_optim.state_dict(),\n",
        "        }\n",
        "        torch.save(checkpoint, filename + \".pth\")\n",
        "\n",
        "    # load model parameters\n",
        "    def load_model(self, filename, for_train=False):\n",
        "        print('Loading models from {}...'.format(filename))\n",
        "        checkpoint = torch.load(filename)\n",
        "        mean = checkpoint.get(\"mean\")\n",
        "        var = checkpoint.get(\"var\")\n",
        "        if mean is not None:\n",
        "            self.mean = mean\n",
        "        if var is not None:\n",
        "            self.var = var\n",
        "        self.policy.load_state_dict(checkpoint[\"policy\"])\n",
        "        self.values.load_state_dict(checkpoint[\"value\"])\n",
        "        if for_train:\n",
        "            self.policy_optim.load_state_dict(checkpoint[\"policy_optim\"])\n",
        "            self.value_optim.load_state_dict(checkpoint[\"value_optim\"])\n",
        "\n",
        "    # load coarse model\n",
        "    def load_coarse_model(self, filename, action_res):\n",
        "        print('Loading coarse models from {}...'.format(filename))\n",
        "        self.coarse_policy = GaussianPolicy(self.num_inputs, action_res,\\\n",
        "            self.upsampled_action_res, False, self.args.coarse2fine_bias).to(self.device)\n",
        "        checkpoint = torch.load(filename)\n",
        "        self.coarse_policy.load_state_dict(checkpoint[\"policy\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVay-cog2Ccj"
      },
      "source": [
        "#A2C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class A2C(object):\n",
        "    def __init__(self, num_inputs, action_space, args):\n",
        "\n",
        "        self.args = args\n",
        "        self.num_inputs = num_inputs #env.observation_space.shape[0]\n",
        "        self.gamma = args.gamma\n",
        "        self.alpha = args.alpha\n",
        "        self.epsilon = args.epsilon\n",
        "        self.action_res = args.action_res\n",
        "        self.automatic_entropy_tuning = args.automatic_entropy_tuning\n",
        "        self.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "        # for reward normalization\n",
        "        self.momentum = args.momentum\n",
        "        self.mean = 0.0\n",
        "        self.var = 1.0\n",
        "\n",
        "        self.entropy_coeff = self.args.entropy_coeff\n",
        "\n",
        "        self.upsampled_action_res = args.action_res * args.action_res_resize\n",
        "        #Value\n",
        "        # self.values = VNetwork(num_inputs, self.action_res, args.hidden_size).to(device=self.device)\n",
        "        self.values = VNetwork(num_inputs, self.action_res, args.hidden_size).to(device=self.device)\n",
        "        self.value_optim = Adam(self.values.parameters(), lr=args.lr)\n",
        "\n",
        "        # policy\n",
        "        self.policy = GaussianPolicy(num_inputs, self.action_res,\\\n",
        "            self.upsampled_action_res, args.residual, args.coarse2fine_bias).to(device=self.device)\n",
        "        self.policy_optim = Adam(self.policy.parameters(), lr=args.lr)\n",
        "\n",
        "        self.policy_scheduler = StepLR(self.policy_optim, step_size=100, gamma=0.99)\n",
        "        self.value_scheduler = StepLR(self.value_optim, step_size=100, gamma=0.99)\n",
        "        # auto alpha\n",
        "        if self.automatic_entropy_tuning:\n",
        "            self.target_entropy = -torch.Tensor([action_space.shape[0]]).to(self.device).item()\n",
        "            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
        "            self.alpha_optim = Adam([self.log_alpha], lr=args.lr)\n",
        "\n",
        "    def select_action(self, state, coarse_action=None, task=None):\n",
        "        mean = None\n",
        "        std = None\n",
        "        state = (torch.FloatTensor(state) / 255.0 * 2.0 - 1.0).to(self.device).unsqueeze(0)\n",
        "        if coarse_action is not None:\n",
        "            coarse_action = torch.FloatTensor(coarse_action).to(self.device).unsqueeze(0)\n",
        "        if task is None or \"shapematch\" not in task:\n",
        "            action, log_prob, mean, std, mask = self.policy.sample(state, coarse_action)\n",
        "        else:\n",
        "            _, log_prob, action, _, mask = self.policy.sample(state, coarse_action)\n",
        "            action = torch.tanh(action)\n",
        "        action = action.detach().cpu().numpy()[0]\n",
        "        if coarse_action is not None:\n",
        "            mask = mask.detach().cpu().numpy()[0]\n",
        "        # log_probs = torch.clamp(log_prob, min=-20, max=0)\n",
        "        return action, mask , log_prob, mean, std\n",
        "\n",
        "\n",
        "    def reward_normalization(self, rewards):\n",
        "    # Clipping the rewards to a certain range can stabilize training\n",
        "        rewards = torch.clamp(rewards, min=-1, max=1)\n",
        "        batch_mean = torch.mean(rewards)\n",
        "        batch_var = torch.var(rewards)\n",
        "        self.mean = self.momentum * self.mean + (1 - self.momentum) * batch_mean\n",
        "        self.var = self.momentum * self.var + (1 - self.momentum) * batch_var\n",
        "        std = torch.sqrt(self.var + 1e-8)\n",
        "        normalized_rewards = (rewards - self.mean) / std\n",
        "        return normalized_rewards\n",
        "\n",
        "\n",
        "    def advantage_normalization(self, advantages):\n",
        "        # update mean and var for reward normalization\n",
        "        batch_mean = torch.mean(advantages)\n",
        "        batch_var = torch.var(advantages)\n",
        "        std = torch.sqrt(batch_var)\n",
        "        normalized_advantages = (advantages - batch_mean) / (std + 1e-8)\n",
        "        return normalized_advantages\n",
        "\n",
        "\n",
        "    def update_parameters(self, memory, updates):\n",
        "        (\n",
        "            state_batch,\n",
        "            action_batch,\n",
        "            reward_batch,\n",
        "            next_state_batch,\n",
        "            mask_batch,\n",
        "            old_log_prob_batch,\n",
        "            reward_l_batch\n",
        "        ) = memory.sample(self.args.batch_size)\n",
        "        state_batch = (torch.FloatTensor(state_batch) / 255.0 * 2.0 - 1.0).to(self.device)\n",
        "        next_state_batch = (torch.FloatTensor(next_state_batch) / 255.0 * 2.0 - 1.0).to(self.device)\n",
        "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
        "        reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
        "        mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
        "        reward_l_batch = torch.FloatTensor(reward_l_batch).to(self.device).unsqueeze(1)\n",
        "\n",
        "        # print(\"reward_l: \", reward_l_batch.cpu().numpy())\n",
        "\n",
        "        reward_batch = self.reward_normalization(reward_batch)\n",
        "\n",
        "        # Calculate the value targets\n",
        "        values_batch = self.values(state_batch)\n",
        "        next_values = self.values(next_state_batch)\n",
        "\n",
        "\n",
        "        returns = reward_l_batch\n",
        "\n",
        "        # Calculate advantages\n",
        "        advantages = reward_l_batch - values_batch\n",
        "        advantages = self.advantage_normalization(advantages)\n",
        "\n",
        "        # Update critic\n",
        "        critic_loss = F.mse_loss(values_batch, returns)\n",
        "\n",
        "        self.value_optim.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        for params in self.values.parameters():\n",
        "            torch.nn.utils.clip_grad_norm_(params, max_norm=10)\n",
        "        self.value_optim.step()\n",
        "        self.value_scheduler.step(critic_loss)\n",
        "\n",
        "        pi, log_pi, _, std, _= self.policy.sample(state_batch)\n",
        "\n",
        "        policy_loss = -(advantages.detach() * log_pi).mean() - self.entropy_coeff * self.policy.entropy(state_batch).mean()\n",
        "\n",
        "        mask_regularize_loss = torch.zeros(1).to(self.device)\n",
        "\n",
        "        self.policy_optim.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        for params in self.policy.parameters():\n",
        "            torch.nn.utils.clip_grad_norm_(params, max_norm=5)\n",
        "        self.policy_optim.step()\n",
        "        self.policy_scheduler.step(policy_loss)\n",
        "\n",
        "        entropy = self.policy.entropy(state_batch)\n",
        "\n",
        "        return critic_loss.item(), policy_loss.item(), -torch.mean(log_pi).item(), self.alpha,\\\n",
        "          torch.norm(std.reshape(std.shape[0], -1), dim=1).mean().item() / (self.args.action_res**2), mask_regularize_loss.item()\n",
        "\n",
        "\n",
        " # save model parameters\n",
        "    def save_model(self, filename):\n",
        "        checkpoint = {\n",
        "            \"mean\": self.mean,\n",
        "            \"var\": self.var,\n",
        "            \"policy\": self.policy.state_dict(),\n",
        "            \"values\": self.values.state_dict(),\n",
        "            \"value_optim\": self.value_optim.state_dict(),\n",
        "            \"policy_optim\": self.policy_optim.state_dict(),\n",
        "        }\n",
        "        torch.save(checkpoint, filename + \".pth\")\n",
        "\n",
        "    def load_model(self, filename, for_train=False):\n",
        "        print('Loading models from {}...'.format(filename))\n",
        "        checkpoint = torch.load(filename)\n",
        "        mean = checkpoint.get(\"mean\")\n",
        "        var = checkpoint.get(\"var\")\n",
        "        if mean is not None:\n",
        "            self.mean = mean\n",
        "        if var is not None:\n",
        "            self.var = var\n",
        "        self.policy.load_state_dict(checkpoint[\"policy\"])\n",
        "        self.values.load_state_dict(checkpoint[\"value\"])\n",
        "        if for_train:\n",
        "            self.policy_optim.load_state_dict(checkpoint[\"policy_optim\"])\n",
        "            self.value_optim.load_state_dict(checkpoint[\"value_optim\"])\n",
        "\n",
        "    def load_coarse_model(self, filename, action_res):\n",
        "        print('Loading coarse models from {}...'.format(filename))\n",
        "        self.coarse_policy = GaussianPolicy(self.num_inputs, action_res,\\\n",
        "            self.upsampled_action_res, False, self.args.coarse2fine_bias).to(self.device)\n",
        "        checkpoint = torch.load(filename)\n",
        "        self.coarse_policy.load_state_dict(checkpoint[\"policy\"])\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "kpobX6mJMEIk",
        "KdSY9o6vMKun",
        "tmu6A0w1B2sq"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
